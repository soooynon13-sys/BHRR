# utils.py
import numpy as np
import torch
import pickle
import os
import glob
from tqdm import tqdm


# ============================================================================
# 1. ê³ ì • ë²”ìœ„ ë¶„ìœ„ìˆ˜ ë§µ ìƒì„± (GPU ê°€ì†)
# ============================================================================

def prepare_quantile_maps_fixed_range_gpu(
    data,
    n_quantiles=500,
    value_min=260,
    value_max=320,
    device='cuda:0',
    batch_size=2000
):
    """
    ê³ ì • ë²”ìœ„ ê¸°ë°˜ ë¶„ìœ„ìˆ˜ ë§µ ìƒì„± (GPU ê°€ì†)
    
    Parameters:
    -----------
    data : xarray.DataArray
        ì…ë ¥ ë°ì´í„° [T, lat, lon] (ë¬¼ë¦¬ ë‹¨ìœ„, e.g., Kelvin)
    n_quantiles : int
        ë¶„ìœ„ìˆ˜ ê°œìˆ˜
    value_min, value_max : float
        ê³ ì • ë²”ìœ„ (ë¬¼ë¦¬ ë‹¨ìœ„)
    device : str
        GPU ë””ë°”ì´ìŠ¤
    batch_size : int
        ë°°ì¹˜ í¬ê¸°
    
    Returns:
    --------
    q_maps_norm : numpy.ndarray
        ì •ê·œí™”ëœ ë¶„ìœ„ìˆ˜ ë§µ [n_quantiles, lat, lon], [0, 1] ë²”ìœ„
    quantiles : numpy.ndarray
        ë¶„ìœ„ìˆ˜ ë°°ì—´ [0, 100]
    """
    
    print(f"\nğŸ”„ Computing quantile maps (GPU: {device})")
    print(f"   Data shape: {data.shape}")
    print(f"   Value range: [{value_min}, {value_max}]")
    print(f"   N quantiles: {n_quantiles}")
    
    T, H, W = data.shape
    quantiles = np.linspace(0, 100, n_quantiles)
    q_levels = torch.linspace(0, 1, n_quantiles, device=device)
    
    # 1. ì •ê·œí™” [0, 1]
    data_norm = (data.values - value_min) / (value_max - value_min)
    data_norm = np.clip(data_norm, 0, 1)  # â­ ë²”ìœ„ í´ë¦¬í•‘
    
    # ë²”ìœ„ ë°– ë°ì´í„° ë¹„ìœ¨
    outside_pct = ((data.values < value_min) | (data.values > value_max)).sum() / data.size * 100
    if outside_pct > 0:
        print(f"   âš ï¸  {outside_pct:.2f}% of data outside range (clipped)")
    
    # 2. ë¶„ìœ„ìˆ˜ ë§µ ê³„ì‚°
    q_maps = np.zeros((n_quantiles, H, W), dtype=np.float32)
    data_2d = data_norm.reshape(T, H * W)
    n_pixels = H * W
    
    with torch.no_grad():
        for start_idx in tqdm(range(0, n_pixels, batch_size), desc="  Computing"):
            end_idx = min(start_idx + batch_size, n_pixels)
            
            for local_idx in range(end_idx - start_idx):
                global_idx = start_idx + local_idx
                lat_idx = global_idx // W
                lon_idx = global_idx % W
                
                pixel_data = data_2d[:, global_idx]
                valid = ~np.isnan(pixel_data)
                
                if valid.sum() >= 10:
                    pixel_tensor = torch.FloatTensor(pixel_data[valid]).to(device)
                    q_vals = torch.quantile(pixel_tensor, q_levels)
                    q_maps[:, lat_idx, lon_idx] = q_vals.cpu().numpy()
                else:
                    # ìœ íš¨ ë°ì´í„° ë¶€ì¡± â†’ ê· ë“± ë¶„í¬
                    q_maps[:, lat_idx, lon_idx] = q_levels.cpu().numpy()
    
    print(f"   âœ… Quantile map: [{q_maps.min():.3f}, {q_maps.max():.3f}]")
    
    return q_maps, quantiles


def prepare_all_quantile_maps_fixed_range(
    lr_train, hr_train,
    lr_val, hr_val,
    n_quantiles=500,
    lr_range=(260, 320),
    hr_range=(260, 320),
    cache_path=None,
    force_recompute=False,
    device='cuda:0',
    batch_size=2000
):
    """
    ì „ì²´ ë¶„ìœ„ìˆ˜ ë§µ ìƒì„± (ê³ ì • ë²”ìœ„, ìºì‹± ì§€ì›)
    
    Parameters:
    -----------
    lr_train, hr_train : xarray.DataArray
        í›ˆë ¨ ë°ì´í„° (ë¬¼ë¦¬ ë‹¨ìœ„)
    lr_val, hr_val : xarray.DataArray
        ê²€ì¦ ë°ì´í„° (ë¬¼ë¦¬ ë‹¨ìœ„)
    n_quantiles : int
        ë¶„ìœ„ìˆ˜ ê°œìˆ˜
    lr_range, hr_range : tuple
        ê³ ì • ë²”ìœ„ (min, max)
    cache_path : str or None
        ìºì‹œ íŒŒì¼ ê²½ë¡œ
    force_recompute : bool
        ê°•ì œ ì¬ê³„ì‚°
    device : str
        GPU ë””ë°”ì´ìŠ¤
    batch_size : int
        ë°°ì¹˜ í¬ê¸°
    
    Returns:
    --------
    result : dict
        {'train': (lr_q, hr_q), 'val': (lr_q, hr_q), 'quantiles': ..., 'normalization': ...}
    """
    
    # ìºì‹œ í™•ì¸
    if cache_path and os.path.exists(cache_path) and not force_recompute:
        print(f"\nğŸ“‚ Loading from cache: {cache_path}")
        with open(cache_path, 'rb') as f:
            result = pickle.load(f)
        print("âœ… Cache loaded")
        return result
    
    print("\n" + "="*60)
    print("ğŸš€ Computing Quantile Maps (Fixed Range)")
    print("="*60)
    
    lr_min, lr_max = lr_range
    hr_min, hr_max = hr_range
    
    print(f"\nğŸ“Š Configuration:")
    print(f"   LR range: [{lr_min}K, {lr_max}K] = [{lr_min-273.15:.1f}Â°C, {lr_max-273.15:.1f}Â°C]")
    print(f"   HR range: [{hr_min}K, {hr_max}K] = [{hr_min-273.15:.1f}Â°C, {hr_max-273.15:.1f}Â°C]")
    print(f"   N quantiles: {n_quantiles}")
    print(f"   Device: {device}")
    
    # ë°ì´í„° ë²”ìœ„ í™•ì¸
    print(f"\nğŸ“Š Data statistics:")
    print(f"   LR train: [{float(lr_train.min()):.2f}, {float(lr_train.max()):.2f}] K")
    print(f"   HR train: [{float(hr_train.min()):.2f}, {float(hr_train.max()):.2f}] K")
    print(f"   LR val:   [{float(lr_val.min()):.2f}, {float(lr_val.max()):.2f}] K")
    print(f"   HR val:   [{float(hr_val.min()):.2f}, {float(hr_val.max()):.2f}] K")
    
    # 1. LR Train
    print(f"\n[1/4] LR Train")
    lr_q_train, quantiles = prepare_quantile_maps_fixed_range_gpu(
        lr_train, n_quantiles, lr_min, lr_max, device, batch_size
    )
    
    # 2. HR Train
    print(f"\n[2/4] HR Train")
    hr_q_train, _ = prepare_quantile_maps_fixed_range_gpu(
        hr_train, n_quantiles, hr_min, hr_max, device, batch_size
    )
    
    # 3. LR Val
    print(f"\n[3/4] LR Val")
    lr_q_val, _ = prepare_quantile_maps_fixed_range_gpu(
        lr_val, n_quantiles, lr_min, lr_max, device, batch_size
    )
    
    # 4. HR Val
    print(f"\n[4/4] HR Val")
    hr_q_val, _ = prepare_quantile_maps_fixed_range_gpu(
        hr_val, n_quantiles, hr_min, hr_max, device, batch_size
    )
    
    # ê²°ê³¼ íŒ¨í‚¤ì§•
    result = {
        'train': (lr_q_train, hr_q_train),
        'val': (lr_q_val, hr_q_val),
        'quantiles': quantiles,
        'normalization': {
            'type': 'fixed_range',
            'lr_min': lr_min,
            'lr_max': lr_max,
            'hr_min': hr_min,
            'hr_max': hr_max
        },
        'metadata': {
            'n_quantiles': n_quantiles,
            'lr_train_shape': lr_train.shape,
            'hr_train_shape': hr_train.shape,
            'lr_val_shape': lr_val.shape,
            'hr_val_shape': hr_val.shape,
            'device': device
        }
    }
    
    # ìºì‹œ ì €ì¥
    if cache_path:
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        print(f"\nğŸ’¾ Saving to cache: {cache_path}")
        with open(cache_path, 'wb') as f:
            pickle.dump(result, f, protocol=pickle.HIGHEST_PROTOCOL)
        print(f"   File size: {os.path.getsize(cache_path) / 1024**2:.2f} MB")
    
    print(f"\nâœ… Quantile maps ready!")
    print(f"   Train: LR={lr_q_train.shape}, HR={hr_q_train.shape}")
    print(f"   Val:   LR={lr_q_val.shape}, HR={hr_q_val.shape}")
    print("="*60 + "\n")
    
    return result


# ============================================================================
# 2. ìºì‹œ ê´€ë¦¬
# ============================================================================

def inspect_quantile_cache(cache_path):
    """
    ë¶„ìœ„ìˆ˜ ë§µ ìºì‹œ ì •ë³´ í™•ì¸
    
    Parameters:
    -----------
    cache_path : str
        ìºì‹œ íŒŒì¼ ê²½ë¡œ
    """
    if not os.path.exists(cache_path):
        print(f"âŒ Cache file not found: {cache_path}")
        return
    
    with open(cache_path, 'rb') as f:
        data = pickle.load(f)
    
    print("\n" + "="*60)
    print(f"Quantile Cache Info")
    print("="*60)
    print(f"File: {cache_path}")
    print(f"Size: {os.path.getsize(cache_path) / 1024**2:.2f} MB")
    print()
    
    # ë¶„ìœ„ìˆ˜ ë§µ
    lr_q_train, hr_q_train = data['train']
    lr_q_val, hr_q_val = data['val']
    
    print(f"Shapes:")
    print(f"  LR train: {lr_q_train.shape}")
    print(f"  HR train: {hr_q_train.shape}")
    print(f"  LR val:   {lr_q_val.shape}")
    print(f"  HR val:   {hr_q_val.shape}")
    print()
    
    print(f"Quantiles: {len(data['quantiles'])} levels")
    print(f"  Range: [{data['quantiles'][0]:.1f}, {data['quantiles'][-1]:.1f}]")
    print()
    
    # ì •ê·œí™” ì •ë³´
    if 'normalization' in data:
        norm = data['normalization']
        print(f"Normalization:")
        print(f"  Type: {norm['type']}")
        if norm['type'] == 'fixed_range':
            print(f"  LR: [{norm['lr_min']}K, {norm['lr_max']}K]")
            print(f"  HR: [{norm['hr_min']}K, {norm['hr_max']}K]")
    else:
        print("Normalization: Not found (old format)")
    print()
    
    # ê°’ ë²”ìœ„
    print(f"Value ranges (normalized):")
    print(f"  LR train: [{lr_q_train.min():.3f}, {lr_q_train.max():.3f}]")
    print(f"  HR train: [{hr_q_train.min():.3f}, {hr_q_train.max():.3f}]")
    print(f"  LR val:   [{lr_q_val.min():.3f}, {lr_q_val.max():.3f}]")
    print(f"  HR val:   [{hr_q_val.min():.3f}, {hr_q_val.max():.3f}]")
    print()
    
    # ë©”íƒ€ë°ì´í„°
    if 'metadata' in data:
        meta = data['metadata']
        print(f"Metadata:")
        for key, value in meta.items():
            print(f"  {key}: {value}")
    
    print("="*60 + "\n")


def list_all_caches(cache_dir='cache'):
    """
    ëª¨ë“  ìºì‹œ íŒŒì¼ ëª©ë¡
    
    Parameters:
    -----------
    cache_dir : str
        ìºì‹œ ë””ë ‰í† ë¦¬
    """
    cache_files = glob.glob(os.path.join(cache_dir, '*.pkl'))
    
    if not cache_files:
        print(f"\nNo cache files in {cache_dir}")
        return
    
    print("\n" + "="*80)
    print(f"Cache Files in {cache_dir}")
    print("="*80)
    
    total_size = 0
    for cache_file in sorted(cache_files):
        size_mb = os.path.getsize(cache_file) / 1024**2
        total_size += size_mb
        print(f"{os.path.basename(cache_file):<50} {size_mb:>10.2f} MB")
    
    print("="*80)
    print(f"{'Total':<50} {total_size:>10.2f} MB")
    print("="*80 + "\n")


def delete_cache(cache_path):
    """
    ìºì‹œ íŒŒì¼ ì‚­ì œ
    
    Parameters:
    -----------
    cache_path : str
        ì‚­ì œí•  ìºì‹œ íŒŒì¼ ê²½ë¡œ
    """
    if os.path.exists(cache_path):
        os.remove(cache_path)
        print(f"ğŸ—‘ï¸  Deleted: {cache_path}")
    else:
        print(f"âŒ Not found: {cache_path}")


def clear_all_caches(cache_dir='cache', confirm=True):
    """
    ëª¨ë“  ìºì‹œ ì‚­ì œ
    
    Parameters:
    -----------
    cache_dir : str
        ìºì‹œ ë””ë ‰í† ë¦¬
    confirm : bool
        í™•ì¸ ì—¬ë¶€
    """
    cache_files = glob.glob(os.path.join(cache_dir, '*.pkl'))
    
    if not cache_files:
        print(f"\nNo cache files in {cache_dir}")
        return
    
    if confirm:
        list_all_caches(cache_dir)
        response = input(f"Delete {len(cache_files)} cache files? (y/n): ")
        if response.lower() != 'y':
            print("Cancelled")
            return
    
    for cache_file in cache_files:
        os.remove(cache_file)
        print(f"ğŸ—‘ï¸  Deleted: {os.path.basename(cache_file)}")
    
    print(f"\nâœ… Deleted {len(cache_files)} files")


# ============================================================================
# 3. GPU ë©”ëª¨ë¦¬ ê´€ë¦¬
# ============================================================================

def print_gpu_memory(device_id=None):
    """
    GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
    
    Parameters:
    -----------
    device_id : int or None
        íŠ¹ì • GPU (Noneì´ë©´ ëª¨ë“  GPU)
    """
    import torch
    
    if not torch.cuda.is_available():
        print("\nâŒ CUDA not available\n")
        return
    
    if device_id is not None:
        devices = [device_id]
    else:
        devices = range(torch.cuda.device_count())
    
    print("\n" + "="*60)
    print("GPU Memory Usage")
    print("="*60)
    
    for i in devices:
        allocated = torch.cuda.memory_allocated(i) / 1024**3
        reserved = torch.cuda.memory_reserved(i) / 1024**3
        total = torch.cuda.get_device_properties(i).total_memory / 1024**3
        
        print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  Total:     {total:.2f} GB")
        print(f"  Allocated: {allocated:.2f} GB ({allocated/total*100:.1f}%)")
        print(f"  Reserved:  {reserved:.2f} GB ({reserved/total*100:.1f}%)")
        print(f"  Free:      {total - allocated:.2f} GB")
    
    print("="*60 + "\n")


def clear_gpu_memory(device_id=None):
    """
    GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    
    Parameters:
    -----------
    device_id : int or None
        íŠ¹ì • GPU (Noneì´ë©´ ëª¨ë“  GPU)
    """
    import torch
    
    if not torch.cuda.is_available():
        print("âŒ CUDA not available")
        return
    
    print("\nğŸ§¹ Clearing GPU memory...")
    
    if device_id is not None:
        with torch.cuda.device(device_id):
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        print(f"âœ… Cleared GPU {device_id}")
    else:
        for i in range(torch.cuda.device_count()):
            with torch.cuda.device(i):
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        print(f"âœ… Cleared all {torch.cuda.device_count()} GPUs")
    
    print_gpu_memory(device_id)


# ============================================================================
# 4. CLI
# ============================================================================

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "list":
            cache_dir = sys.argv[2] if len(sys.argv) > 2 else 'cache'
            list_all_caches(cache_dir)
        
        elif command == "inspect":
            if len(sys.argv) < 3:
                print("Usage: python utils.py inspect <cache_file>")
            else:
                inspect_quantile_cache(sys.argv[2])
        
        elif command == "delete":
            if len(sys.argv) < 3:
                print("Usage: python utils.py delete <cache_file>")
            else:
                delete_cache(sys.argv[2])
        
        elif command == "clear":
            cache_dir = sys.argv[2] if len(sys.argv) > 2 else 'cache'
            clear_all_caches(cache_dir, confirm=True)
        
        elif command == "gpu":
            device_id = int(sys.argv[2]) if len(sys.argv) > 2 else None
            print_gpu_memory(device_id)
        
        elif command == "clear-gpu":
            device_id = int(sys.argv[2]) if len(sys.argv) > 2 else None
            clear_gpu_memory(device_id)
        
        else:
            print(f"Unknown command: {command}")
            print("\nAvailable commands:")
            print("  list [cache_dir]         - List all cache files")
            print("  inspect <file>           - Inspect cache file")
            print("  delete <file>            - Delete cache file")
            print("  clear [cache_dir]        - Clear all caches")
            print("  gpu [device_id]          - Show GPU memory")
            print("  clear-gpu [device_id]    - Clear GPU memory")
    
    else:
        print("\n" + "="*60)
        print("Utils - Cache & GPU Management")
        print("="*60)
        print("\nUsage: python utils.py <command> [args]")
        print("\nCommands:")
        print("  list [cache_dir]         - List all cache files")
        print("  inspect <file>           - Inspect cache file details")
        print("  delete <file>            - Delete specific cache file")
        print("  clear [cache_dir]        - Clear all cache files")
        print("  gpu [device_id]          - Show GPU memory usage")
        print("  clear-gpu [device_id]    - Clear GPU memory cache")
        print("\nExamples:")
        print("  python utils.py list")
        print("  python utils.py inspect cache/quantile_maps.pkl")
        print("  python utils.py clear cache")
        print("  python utils.py gpu 0")
        print("  python utils.py clear-gpu")
        print("="*60 + "\n")