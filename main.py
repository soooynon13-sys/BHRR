# main.py
"""
Complete Bias Correction Pipeline (Fixed Range)

Workflow:
1. Data Preparation - Historical ë¶„ìœ„ìˆ˜ ë§µ ìƒì„±
2. Model Training - ViT í•™ìŠµ
3. Inference - SSP ì‹œë‚˜ë¦¬ì˜¤ ë³´ì •
"""

import os
import argparse
import xarray as xr
from pathlib import Path

# ìš°ë¦¬ ëª¨ë“ˆë“¤
from utils import (
    prepare_all_quantile_maps_fixed_range,
    inspect_quantile_cache,
    print_gpu_memory,
    clear_gpu_memory
)
from train import train_fixed_range, get_default_config
from inference2 import run_bias_correction_fixed_range, run_multiple_scenarios
from model import SimpleViT


def setup_directories(base_dir='./'):
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    dirs = {
        'cache': os.path.join(base_dir, 'cache'),
        'checkpoints': os.path.join(base_dir, 'checkpoints/vit_fixed_range'),
        'plots': os.path.join(base_dir, 'plots/vit_fixed_range'),
        'outputs': os.path.join(base_dir, 'outputs')
    }
    
    for name, path in dirs.items():
        os.makedirs(path, exist_ok=True)
        print(f"âœ… {name:12s}: {path}")
    
    return dirs


def step1_prepare_data(
    gcm_hist_path,
    obs_hist_path,
    dirs,
    n_quantiles=500,
    lr_range=(260, 320),
    hr_range=(260, 320),
    device='cuda:0',
    force_recompute=False
):
    """
    Step 1: ë°ì´í„° ì¤€ë¹„ - Historical ë¶„ìœ„ìˆ˜ ë§µ ìƒì„±
    
    Parameters:
    -----------
    gcm_hist_path : str
        Historical GCM ë°ì´í„° ê²½ë¡œ
    obs_hist_path : str
        Historical OBS ë°ì´í„° ê²½ë¡œ
    dirs : dict
        ë””ë ‰í† ë¦¬ ì •ë³´
    n_quantiles : int
        ë¶„ìœ„ìˆ˜ ê°œìˆ˜
    lr_range, hr_range : tuple
        ê³ ì • ë²”ìœ„ (min, max)
    device : str
        GPU ë””ë°”ì´ìŠ¤
    force_recompute : bool
        ê°•ì œ ì¬ê³„ì‚°
    
    Returns:
    --------
    cache_path : str
        ìƒì„±ëœ ìºì‹œ íŒŒì¼ ê²½ë¡œ
    """
    
    print("\n" + "="*80)
    print("STEP 1: DATA PREPARATION")
    print("="*80)
    
    # ìºì‹œ ê²½ë¡œ
    cache_filename = f'quantile_maps_q{n_quantiles}_{lr_range[0]}_{lr_range[1]}.pkl'
    cache_path = os.path.join(dirs['cache'], cache_filename)
    
    # ìºì‹œ í™•ì¸
    if os.path.exists(cache_path) and not force_recompute:
        print(f"\nâœ… Cache exists: {cache_path}")
        inspect_quantile_cache(cache_path)
        return cache_path
    
    # ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Loading data...")
    print(f"   GCM Historical: {gcm_hist_path}")
    print(f"   OBS Historical: {obs_hist_path}")
    
    gcm_hist = xr.open_dataset(gcm_hist_path)
    obs_hist = xr.open_dataset(obs_hist_path)
    
    # ë³€ìˆ˜ ì´ë¦„ ê°ì§€
    gcm_var = [v for v in gcm_hist.data_vars if gcm_hist[v].ndim == 3][0]
    obs_var = [v for v in obs_hist.data_vars if obs_hist[v].ndim == 3][0]
    
    print(f"   GCM variable: {gcm_var}")
    print(f"   OBS variable: {obs_var}")
    
    # Train/Val ë¶„í•  (80/20)
    print(f"\nâœ‚ï¸  Splitting train/val (80/20)...")
    
    gcm_times = gcm_hist.time.values
    obs_times = obs_hist.time.values
    
    split_idx_gcm = int(len(gcm_times) * 0.8)
    split_idx_obs = int(len(obs_times) * 0.8)
    
    lr_train = gcm_hist[gcm_var].isel(time=slice(0, split_idx_gcm))
    lr_val = gcm_hist[gcm_var].isel(time=slice(split_idx_gcm, None))
    hr_train = obs_hist[obs_var].isel(time=slice(0, split_idx_obs))
    hr_val = obs_hist[obs_var].isel(time=slice(split_idx_obs, None))
    
    print(f"   LR train: {lr_train.shape} ({lr_train.time.values[0]} ~ {lr_train.time.values[-1]})")
    print(f"   LR val:   {lr_val.shape} ({lr_val.time.values[0]} ~ {lr_val.time.values[-1]})")
    print(f"   HR train: {hr_train.shape} ({hr_train.time.values[0]} ~ {hr_train.time.values[-1]})")
    print(f"   HR val:   {hr_val.shape} ({hr_val.time.values[0]} ~ {hr_val.time.values[-1]})")
    
    # ë¶„ìœ„ìˆ˜ ë§µ ìƒì„±
    print(f"\nğŸš€ Computing quantile maps...")
    result = prepare_all_quantile_maps_fixed_range(
        lr_train, hr_train,
        lr_val, hr_val,
        n_quantiles=n_quantiles,
        lr_range=lr_range,
        hr_range=hr_range,
        cache_path=cache_path,
        force_recompute=force_recompute,
        device=device,
        batch_size=2000
    )
    
    print(f"\nâœ… Data preparation complete!")
    print(f"   Cache saved: {cache_path}")
    
    # ìºì‹œ ê²€ì‚¬
    inspect_quantile_cache(cache_path)
    
    return cache_path


def step2_train_model(
    cache_path,
    dirs,
    config=None,
    device='cuda:1',
    resume_from=None
):
    """
    Step 2: ëª¨ë¸ í•™ìŠµ
    
    Parameters:
    -----------
    cache_path : str
        ë¶„ìœ„ìˆ˜ ë§µ ìºì‹œ ê²½ë¡œ
    dirs : dict
        ë””ë ‰í† ë¦¬ ì •ë³´
    config : dict or None
        í•™ìŠµ ì„¤ì • (Noneì´ë©´ ê¸°ë³¸ê°’)
    device : str
        GPU ë””ë°”ì´ìŠ¤
    resume_from : str or None
        ì¬ê°œí•  ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    
    Returns:
    --------
    checkpoint_path : str
        í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    """
    
    print("\n" + "="*80)
    print("STEP 2: MODEL TRAINING")
    print("="*80)
    
    # ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Loading quantile maps from: {cache_path}")
    import pickle
    with open(cache_path, 'rb') as f:
        qdata = pickle.load(f)
    
    lr_q_train, hr_q_train = qdata['train']
    lr_q_val, hr_q_val = qdata['val']
    quantiles = qdata['quantiles']
    normalization = qdata['normalization']
    
    print(f"   Train: LR={lr_q_train.shape}, HR={hr_q_train.shape}")
    print(f"   Val:   LR={lr_q_val.shape}, HR={hr_q_val.shape}")
    print(f"   Normalization: {normalization['type']}")
    
    # ì„¤ì •
    if config is None:
        config = get_default_config(use_fixed_range=True)
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    config['checkpoint_dir'] = dirs['checkpoints']
    config['plot_dir'] = dirs['plots']
    config['device'] = device
    
    print(f"\nâš™ï¸  Training configuration:")
    for key, value in config.items():
        print(f"   {key:20s}: {value}")
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    print_gpu_memory(int(device.split(':')[1]) if ':' in device else None)
    
    # í•™ìŠµ
    print(f"\nğŸš€ Starting training...")
    
    if resume_from:
        print(f"   Resuming from: {resume_from}")
        # TODO: resume ë¡œì§ êµ¬í˜„
    
    trainer = train_fixed_range(
        lr_q_train, hr_q_train,
        lr_q_val, hr_q_val,
        quantiles=quantiles,
        lr_range=(normalization['lr_min'], normalization['lr_max']),
        hr_range=(normalization['hr_min'], normalization['hr_max']),
        custom_config=config
    )
    
    checkpoint_path = os.path.join(dirs['checkpoints'], 'best_model.pth')
    
    print(f"\nâœ… Training complete!")
    print(f"   Best loss: {trainer.best_loss:.6f}")
    print(f"   Checkpoint: {checkpoint_path}")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_gpu_memory(int(device.split(':')[1]) if ':' in device else None)
    
    return checkpoint_path


def step3_inference(
    ssp_paths,
    checkpoint_path,
    cache_path,
    dirs,
    device='cuda:1',
    batch_size=32,
    var_name='tasmax'
):
    """
    Step 3: SSP ì‹œë‚˜ë¦¬ì˜¤ ë³´ì •
    
    Parameters:
    -----------
    ssp_paths : dict
        SSP ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ê²½ë¡œ {'ssp245': '/path/to/ssp245.nc', ...}
    checkpoint_path : str
        ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    cache_path : str
        ë¶„ìœ„ìˆ˜ ë§µ ìºì‹œ ê²½ë¡œ
    dirs : dict
        ë””ë ‰í† ë¦¬ ì •ë³´
    device : str
        GPU ë””ë°”ì´ìŠ¤
    batch_size : int
        ë°°ì¹˜ í¬ê¸°
    var_name : str
        ë³€ìˆ˜ ì´ë¦„
    
    Returns:
    --------
    results : dict
        {scenario: corrected_data}
    """
    
    print("\n" + "="*80)
    print("STEP 3: INFERENCE (SSP SCENARIOS)")
    print("="*80)
    
    # ëª¨ë¸ ì„¤ì •
    print(f"\nâš™ï¸  Model configuration:")
    model_config = {
        'img_size': 240,
        'patch_size': 8,
        'in_chans': 1,
        'out_chans': 1,
        'embed_dim': 512,
        'depth': 12,
        'num_heads': 8,
        'dropout': 0.1,
        'use_sigmoid': True  # â­ ê³ ì • ë²”ìœ„
    }
    
    for key, value in model_config.items():
        print(f"   {key:20s}: {value}")
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    print_gpu_memory(int(device.split(':')[1]) if ':' in device else None)
    
    # ê° ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬
    results = {}
    
    for i, (scenario, input_path) in enumerate(ssp_paths.items(), 1):
        print(f"\n{'='*80}")
        print(f"Processing {i}/{len(ssp_paths)}: {scenario}")
        print(f"{'='*80}")
        
        if not os.path.exists(input_path):
            print(f"âš ï¸  Skipping: Input file not found")
            continue
        
        output_path = os.path.join(dirs['outputs'], f'vit_{scenario}_corrected.nc')
        
        try:
            corrected = run_bias_correction_fixed_range(
                lr_data_path=input_path,
                checkpoint_path=checkpoint_path,
                quantile_map_path=cache_path,
                output_path=output_path,
                model_class=SimpleViT,
                model_config=model_config,
                device=device,
                batch_size=batch_size,
                var_name=var_name
            )
            
            results[scenario] = corrected
            print(f"âœ… {scenario} completed!")
            
        except Exception as e:
            print(f"âŒ {scenario} failed: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print(f"\nâœ… Inference complete!")
    print(f"   Processed: {len(results)}/{len(ssp_paths)} scenarios")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    clear_gpu_memory(int(device.split(':')[1]) if ':' in device else None)
    
    return results


def run_full_pipeline(
    gcm_hist_path,
    obs_hist_path,
    ssp_paths,
    base_dir='./',
    n_quantiles=500,
    lr_range=(260, 320),
    hr_range=(260, 320),
    num_epochs=200,
    data_device='cuda:0',
    train_device='cuda:1',
    inference_device='cuda:1',
    skip_data_prep=False,
    skip_training=False,
    force_recompute=False
):
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    
    Parameters:
    -----------
    gcm_hist_path : str
        Historical GCM ë°ì´í„°
    obs_hist_path : str
        Historical OBS ë°ì´í„°
    ssp_paths : dict
        SSP ì‹œë‚˜ë¦¬ì˜¤ë“¤ {'ssp245': '/path/to/ssp245.nc', ...}
    base_dir : str
        ì‘ì—… ë””ë ‰í† ë¦¬
    n_quantiles : int
        ë¶„ìœ„ìˆ˜ ê°œìˆ˜
    lr_range, hr_range : tuple
        ê³ ì • ë²”ìœ„
    num_epochs : int
        í•™ìŠµ ì—í­
    data_device : str
        ë°ì´í„° ì¤€ë¹„ìš© GPU
    train_device : str
        í•™ìŠµìš© GPU
    inference_device : str
        ì¶”ë¡ ìš© GPU
    skip_data_prep : bool
        ë°ì´í„° ì¤€ë¹„ ê±´ë„ˆë›°ê¸°
    skip_training : bool
        í•™ìŠµ ê±´ë„ˆë›°ê¸°
    force_recompute : bool
        ê°•ì œ ì¬ê³„ì‚°
    
    Returns:
    --------
    results : dict
        ì „ì²´ ê²°ê³¼
    """
    
    print("\n" + "="*80)
    print("ğŸš€ FULL BIAS CORRECTION PIPELINE")
    print("="*80)
    print(f"\nInput files:")
    print(f"  GCM Historical: {gcm_hist_path}")
    print(f"  OBS Historical: {obs_hist_path}")
    print(f"  SSP Scenarios:")
    for scenario, path in ssp_paths.items():
        print(f"    {scenario:10s}: {path}")
    print(f"\nConfiguration:")
    print(f"  Base directory: {base_dir}")
    print(f"  N quantiles:    {n_quantiles}")
    print(f"  LR range:       {lr_range} K")
    print(f"  HR range:       {hr_range} K")
    print(f"  Num epochs:     {num_epochs}")
    print(f"  Data device:    {data_device}")
    print(f"  Train device:   {train_device}")
    print(f"  Infer device:   {inference_device}")
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    print(f"\nğŸ“ Setting up directories...")
    dirs = setup_directories(base_dir)
    
    # Step 1: ë°ì´í„° ì¤€ë¹„
    if not skip_data_prep:
        cache_path = step1_prepare_data(
            gcm_hist_path=gcm_hist_path,
            obs_hist_path=obs_hist_path,
            dirs=dirs,
            n_quantiles=n_quantiles,
            lr_range=lr_range,
            hr_range=hr_range,
            device=data_device,
            force_recompute=force_recompute
        )
    else:
        print(f"\nâ­ï¸  Skipping data preparation...")
        cache_filename = f'quantile_maps_q{n_quantiles}_{lr_range[0]}_{lr_range[1]}.pkl'
        cache_path = os.path.join(dirs['cache'], cache_filename)
        if not os.path.exists(cache_path):
            raise FileNotFoundError(f"Cache not found: {cache_path}")
        print(f"   Using existing cache: {cache_path}")
    
    # Step 2: í•™ìŠµ
    if not skip_training:
        config = get_default_config(use_fixed_range=True)
        config['num_epochs'] = num_epochs
        
        checkpoint_path = step2_train_model(
            cache_path=cache_path,
            dirs=dirs,
            config=config,
            device=train_device
        )
    else:
        print(f"\nâ­ï¸  Skipping training...")
        checkpoint_path = os.path.join(dirs['checkpoints'], 'best_model.pth')
        if not os.path.exists(checkpoint_path):
            raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
        print(f"   Using existing checkpoint: {checkpoint_path}")
    
    # Step 3: ì¶”ë¡ 
    results = step3_inference(
        ssp_paths=ssp_paths,
        checkpoint_path=checkpoint_path,
        cache_path=cache_path,
        dirs=dirs,
        device=inference_device,
        batch_size=32,
        var_name='tasmax'
    )
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*80)
    print("ğŸ‰ PIPELINE COMPLETED!")
    print("="*80)
    print(f"\nğŸ“Š Summary:")
    print(f"   Data prepared:  {cache_path}")
    print(f"   Model trained:  {checkpoint_path}")
    print(f"   Scenarios processed: {len(results)}/{len(ssp_paths)}")
    
    if results:
        print(f"\nğŸ“‚ Output files:")
        for scenario in results.keys():
            output_path = os.path.join(dirs['outputs'], f'vit_{scenario}_corrected.nc')
            if os.path.exists(output_path):
                size_mb = os.path.getsize(output_path) / 1024**2
                print(f"   {scenario:10s}: {output_path} ({size_mb:.2f} MB)")
    
    print("\n" + "="*80 + "\n")
    
    return {
        'cache_path': cache_path,
        'checkpoint_path': checkpoint_path,
        'results': results,
        'dirs': dirs
    }


