# inference.py
import torch
import numpy as np
import xarray as xr
from tqdm import tqdm
from scipy import interpolate
import os


def load_trained_model(checkpoint_path, model_class, device='cuda:0'):
    """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ“¥ Loading checkpoint from: {checkpoint_path}")
    
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    print(f"   Epoch: {checkpoint['epoch']}")
    print(f"   Val Loss: {checkpoint['val_loss']:.6f}")
    
    return checkpoint


def prepare_inference_quantile_maps_gpu(
    lr_data,
    n_quantiles,
    lr_mean,
    lr_std,
    device='cuda:0',
    batch_size=2000
):
    """
    GPU ê°€ì† ì¶”ë¡ ìš© ë¶„ìœ„ìˆ˜ ë§µ ìƒì„±
    
    Parameters:
    -----------
    lr_data : xarray.DataArray
        ì˜ˆì¸¡í•  LR ë°ì´í„° [T, lat, lon]
    n_quantiles : int
        ë¶„ìœ„ìˆ˜ ê°œìˆ˜
    lr_mean, lr_std : float
        í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ ì •ê·œí™” í†µê³„
    device : str
        GPU ë””ë°”ì´ìŠ¤
    batch_size : int
        ë°°ì¹˜ í¬ê¸°
    
    Returns:
    --------
    lr_q_normalized : numpy.ndarray
        ì •ê·œí™”ëœ ë¶„ìœ„ìˆ˜ ë§µ [n_quantiles, lat, lon]
    """
    print(f"\nğŸ”„ Computing quantile maps for inference (GPU)...")
    print(f"   Data shape: {lr_data.shape}")
    print(f"   Device: {device}")
    
    T, H, W = lr_data.shape
    quantiles = np.linspace(0, 100, n_quantiles)
    
    # ë¶„ìœ„ìˆ˜ ë ˆë²¨ì„ GPUë¡œ
    q_levels = torch.linspace(0, 1, n_quantiles, device=device)
    
    # ê²°ê³¼ ë°°ì—´
    lr_q_maps = np.zeros((n_quantiles, H, W), dtype=np.float32)
    
    # ë°ì´í„° reshape [T, H*W]
    data_2d = lr_data.values.reshape(T, H * W)
    
    n_pixels = H * W
    
    with torch.no_grad():
        for start_idx in tqdm(range(0, n_pixels, batch_size), desc="Computing quantiles (GPU)"):
            end_idx = min(start_idx + batch_size, n_pixels)
            
            # ë°°ì¹˜ ì¶”ì¶œ
            batch_data = data_2d[:, start_idx:end_idx]
            
            # ê° í”½ì…€ë³„ë¡œ ì²˜ë¦¬
            for local_idx in range(batch_data.shape[1]):
                pixel_data = batch_data[:, local_idx]
                valid = ~np.isnan(pixel_data)
                
                if valid.sum() >= 10:
                    # GPUë¡œ ì´ë™ ë° ë¶„ìœ„ìˆ˜ ê³„ì‚°
                    pixel_tensor = torch.FloatTensor(pixel_data[valid]).to(device)
                    q_vals = torch.quantile(pixel_tensor, q_levels)
                    
                    # ê²°ê³¼ ì €ì¥
                    global_idx = start_idx + local_idx
                    lat_idx = global_idx // W
                    lon_idx = global_idx % W
                    
                    lr_q_maps[:, lat_idx, lon_idx] = q_vals.cpu().numpy()
                else:
                    # ìœ íš¨ ë°ì´í„° ë¶€ì¡±
                    global_idx = start_idx + local_idx
                    lat_idx = global_idx // W
                    lon_idx = global_idx % W
                    lr_q_maps[:, lat_idx, lon_idx] = np.nan
    
    # ì •ê·œí™” (í›ˆë ¨ ì‹œì™€ ë™ì¼)
    lr_q_normalized = (lr_q_maps - lr_mean) / lr_std
    lr_q_normalized = np.nan_to_num(lr_q_normalized, 0)
    
    print(f"âœ… Quantile maps ready: {lr_q_normalized.shape}")
    return lr_q_normalized, quantiles


def predict_quantile_maps(model, lr_q_normalized, device, batch_size=16):
    """
    ëª¨ë¸ë¡œ HR ë¶„ìœ„ìˆ˜ ë§µ ì˜ˆì¸¡
    
    Parameters:
    -----------
    model : torch.nn.Module
        í•™ìŠµëœ ëª¨ë¸
    lr_q_normalized : numpy.ndarray
        ì •ê·œí™”ëœ LR ë¶„ìœ„ìˆ˜ ë§µ [n_quantiles, lat, lon]
    device : str
        ë””ë°”ì´ìŠ¤
    batch_size : int
        ë°°ì¹˜ í¬ê¸°
    
    Returns:
    --------
    hr_q_predicted : numpy.ndarray
        ì˜ˆì¸¡ëœ HR ë¶„ìœ„ìˆ˜ ë§µ
    """
    print(f"\nğŸ¤– Predicting with model...")
    
    model.eval()
    hr_q_predicted = []
    
    n_quantiles = len(lr_q_normalized)
    
    with torch.no_grad():
        for i in tqdm(range(0, n_quantiles, batch_size), desc="Predicting"):
            batch = lr_q_normalized[i:i+batch_size]
            batch_tensor = torch.FloatTensor(batch).unsqueeze(1).to(device)
            
            with torch.cuda.amp.autocast():
                pred = model(batch_tensor)
            
            hr_q_predicted.append(pred.cpu().squeeze(1).numpy())
    
    hr_q_predicted = np.concatenate(hr_q_predicted, axis=0)
    
    print(f"âœ… Prediction done: {hr_q_predicted.shape}")
    return hr_q_predicted


def apply_quantile_correction_gpu(
    lr_data,
    hr_q_predicted,
    quantiles,
    hr_mean,
    hr_std,
    device='cuda:0',
    batch_size=1000
):
    """
    GPU ê°€ì† ë¶„ìœ„ìˆ˜ ë³´ì • ì ìš©
    
    Parameters:
    -----------
    lr_data : xarray.DataArray
        ì›ë³¸ LR ë°ì´í„° [T, lat, lon]
    hr_q_predicted : numpy.ndarray
        ì˜ˆì¸¡ëœ HR ë¶„ìœ„ìˆ˜ ë§µ (ì •ê·œí™”ë¨)
    quantiles : numpy.ndarray
        ë¶„ìœ„ìˆ˜ ë°°ì—´
    hr_mean, hr_std : float
        HR ì •ê·œí™” í†µê³„ (ì—­ì •ê·œí™”ìš©)
    device : str
        GPU ë””ë°”ì´ìŠ¤
    batch_size : int
        ë°°ì¹˜ í¬ê¸°
    
    Returns:
    --------
    lr_corrected : xarray.DataArray
        ë³´ì •ëœ ë°ì´í„°
    """
    print(f"\nğŸ”§ Applying bias correction (GPU)...")
    
    # HR ë¶„ìœ„ìˆ˜ ë§µ ì—­ì •ê·œí™”
    hr_q_maps = hr_q_predicted * hr_std + hr_mean
    
    # ë³´ì •ëœ ë°ì´í„° ì´ˆê¸°í™”
    lr_corrected = lr_data.copy(deep=True)
    
    T, H, W = lr_data.shape
    n_pixels = H * W
    
    # ë°ì´í„° reshape
    data_2d = lr_data.values.reshape(T, H * W)
    corrected_2d = np.zeros_like(data_2d)
    
    # GPU í…ì„œë¡œ ë³€í™˜
    q_levels = torch.FloatTensor(quantiles / 100.0).to(device)  # 0~1 ë²”ìœ„
    
    with torch.no_grad():
        for start_idx in tqdm(range(0, n_pixels, batch_size), desc="Correcting (GPU)"):
            end_idx = min(start_idx + batch_size, n_pixels)
            
            # ë°°ì¹˜ ì²˜ë¦¬
            for local_idx in range(end_idx - start_idx):
                global_idx = start_idx + local_idx
                lat_idx = global_idx // W
                lon_idx = global_idx % W
                
                lr_vals = data_2d[:, global_idx]
                hr_q = hr_q_maps[:, lat_idx, lon_idx]
                
                valid = ~np.isnan(lr_vals)
                if valid.sum() < 10:
                    continue
                
                # LR ë¶„ìœ„ìˆ˜ ê³„ì‚° (GPU)
                lr_vals_gpu = torch.FloatTensor(lr_vals[valid]).to(device)
                lr_q_gpu = torch.quantile(lr_vals_gpu, q_levels)
                
                # HR ë¶„ìœ„ìˆ˜ (GPU)
                hr_q_gpu = torch.FloatTensor(hr_q).to(device)
                
                # ë³´ê°„ (CPUê°€ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ)
                lr_q_cpu = lr_q_gpu.cpu().numpy()
                hr_q_cpu = hr_q_gpu.cpu().numpy()
                
                # ë³´ê°„ í•¨ìˆ˜ ìƒì„±
                transfer = interpolate.interp1d(
                    lr_q_cpu, hr_q_cpu,
                    kind='linear',
                    bounds_error=False,
                    fill_value=(hr_q_cpu[0], hr_q_cpu[-1])
                )
                
                # ë³´ì • ì ìš©
                corrected_2d[:, global_idx] = transfer(lr_vals)
    
    # Reshape back
    lr_corrected.values = corrected_2d.reshape(T, H, W)
    
    print(f"âœ… Bias correction complete!")
    return lr_corrected


def run_bias_correction(
    lr_data_path,
    checkpoint_path,
    output_path,
    model_class,
    model_config,
    n_quantiles=100,
    device='cuda:1',
    batch_size=16,
    restore_extremes=False,
    use_gpu_quantiles=True,  # â­ GPU ë¶„ìœ„ìˆ˜ ê³„ì‚° ì˜µì…˜
    quantile_batch_size=2000,  # â­ ë¶„ìœ„ìˆ˜ ê³„ì‚° ë°°ì¹˜ í¬ê¸°
    var_name=None, 
    quantile_map_path=None # pkl íŒŒì¼ ê²½ë¡œ
):
    """
    ì „ì²´ í¸ì´ë³´ì • íŒŒì´í”„ë¼ì¸ (GPU ê°€ì†)
    
    Parameters:
    -----------
    lr_data_path : str
        ì…ë ¥ LR ë°ì´í„° ê²½ë¡œ (.nc)
    checkpoint_path : str
        ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    output_path : str
        ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    model_class : class
        ëª¨ë¸ í´ë˜ìŠ¤
    model_config : dict
        ëª¨ë¸ ì„¤ì •
    n_quantiles : int
        ë¶„ìœ„ìˆ˜ ê°œìˆ˜
    device : str
        ë””ë°”ì´ìŠ¤
    batch_size : int
        ëª¨ë¸ ì¶”ë¡  ë°°ì¹˜ í¬ê¸°
    restore_extremes : bool
        ê·¹ê°’ ë³µì› ì—¬ë¶€
    use_gpu_quantiles : bool
        ë¶„ìœ„ìˆ˜ ê³„ì‚°ì— GPU ì‚¬ìš© ì—¬ë¶€
    quantile_batch_size : int
        ë¶„ìœ„ìˆ˜ ê³„ì‚° ë°°ì¹˜ í¬ê¸°
    """
    
    print("="*60)
    print("ğŸš€ Starting Bias Correction Pipeline (GPU Accelerated)")
    print("="*60)
    
    # 1. ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = load_trained_model(checkpoint_path, model_class, device)
    
    lr_mean, lr_std = checkpoint['lr_stats']
    hr_mean, hr_std = checkpoint['hr_stats']
    
    print(f"\nğŸ“Š Normalization stats:")
    print(f"   LR: mean={lr_mean:.4f}, std={lr_std:.4f}")
    print(f"   HR: mean={hr_mean:.4f}, std={hr_std:.4f}")
    
    # 2. ëª¨ë¸ ì´ˆê¸°í™”
    print(f"\nğŸ”§ Initializing model...")
    model = model_class(**model_config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print(f"   Parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M")
    
    # 3. LR ë°ì´í„° ë¡œë“œ
    print(f"\nğŸ“‚ Loading LR data from: {lr_data_path}")
    lr_ds = xr.open_dataset(lr_data_path)
    
    # ë³€ìˆ˜ ì´ë¦„ ìë™ ê°ì§€
    if var_name is None:
        var_candidates = [var for var in lr_ds.data_vars if lr_ds[var].ndim == 3]
        if not var_candidates:
            raise ValueError("No suitable 3D variable found in the dataset.")
        var_name = var_candidates[0]
        print(f"   Detected variable: {var_name}")
    lr_data = lr_ds[var_name]
    
    print(f"   Variable: {var_name}")
    print(f"   Shape: {lr_data.shape}")
    print(f"   Time range: {lr_data.time.values[0]} ~ {lr_data.time.values[-1]}")
    if quantile_map_path is not None and os.path.exists(quantile_map_path):
        print(f"   Loading precomputed quantile maps from: {quantile_map_path}")
        import pickle
        with open(quantile_map_path, 'rb') as f:
            quantile_data = pickle.load(f)
        lr_q_normalized = quantile_data['train'][0]
        quantiles = quantile_data['quantiles']
    else:
        print(f"   No precomputed quantile maps found.")
        print(f"   Computing quantile maps...")
        # 4. ë¶„ìœ„ìˆ˜ ë§µ ìƒì„± (GPU ê°€ì†)
        if use_gpu_quantiles:
            lr_q_normalized, quantiles = prepare_inference_quantile_maps_gpu(
                lr_data, n_quantiles, lr_mean, lr_std,
                device=device, batch_size=quantile_batch_size
            )
        else:
            # CPU ë²„ì „ (ê¸°ì¡´ ì½”ë“œ)
            from inference import prepare_inference_quantile_maps
            lr_q_normalized, quantiles = prepare_inference_quantile_maps(
                lr_data, n_quantiles, lr_mean, lr_std
            )
    
    # 5. ëª¨ë¸ ì˜ˆì¸¡
    hr_q_predicted = predict_quantile_maps(
        model, lr_q_normalized, device, batch_size
    )
    
    # 6. í¸ì´ë³´ì • ì ìš© (GPU ê°€ì†)
    if use_gpu_quantiles:
        lr_corrected = apply_quantile_correction_gpu(
            lr_data, hr_q_predicted, quantiles, hr_mean, hr_std,
            device=device, batch_size=quantile_batch_size
        )
    else:
        # CPU ë²„ì „
        from inference import apply_quantile_correction
        lr_corrected = apply_quantile_correction(
            lr_data, hr_q_predicted, quantiles, hr_mean, hr_std
        )
    
    # 7. ê·¹ê°’ ë³µì› (ì„ íƒ)
    if restore_extremes:
        from inference import restore_extreme_values
        target_stats = {
            'mean': hr_mean,
            'std': hr_std,
            'q01': hr_mean - 3 * hr_std,
            'q99': hr_mean + 3 * hr_std
        }
        lr_corrected = restore_extreme_values(lr_corrected, target_stats)
    
    # 8. ì €ì¥
    print(f"\nğŸ’¾ Saving corrected data to: {output_path}")
    
    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    lr_corrected.attrs['bias_correction'] = 'Applied'
    lr_corrected.attrs['model'] = model_class.__name__
    lr_corrected.attrs['checkpoint'] = checkpoint_path
    lr_corrected.attrs['n_quantiles'] = n_quantiles
    
    # Datasetìœ¼ë¡œ ë³€í™˜ í›„ ì €ì¥
    output_ds = lr_corrected.to_dataset(name=var_name + '_corrected')
    output_ds.to_netcdf(output_path)
    
    print(f"âœ… Saved successfully!")
    
    # 9. í†µê³„ ë¹„êµ
    print(f"\nğŸ“Š Statistics Comparison:")
    print(f"{'Metric':<15} {'Original':<15} {'Corrected':<15}")
    print(f"{'-'*45}")
    print(f"{'Mean':<15} {float(lr_data.mean()):<15.4f} {float(lr_corrected.mean()):<15.4f}")
    print(f"{'Std':<15} {float(lr_data.std()):<15.4f} {float(lr_corrected.std()):<15.4f}")
    print(f"{'Min':<15} {float(lr_data.min()):<15.4f} {float(lr_corrected.min()):<15.4f}")
    print(f"{'Max':<15} {float(lr_data.max()):<15.4f} {float(lr_corrected.max()):<15.4f}")
    
    print("\n" + "="*60)
    print("ğŸ‰ Bias Correction Completed!")
    print("="*60)
    
    return lr_corrected


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    from model import SimpleViT
    
    model_config = {
        'img_size': 240,
        'patch_size': 8,
        'in_chans': 1,
        'out_chans': 1,
        'embed_dim': 512,
        'depth': 12,
        'num_heads': 8,
        'dropout': 0.1
    }
    
    lr_corrected = run_bias_correction(
        lr_data_path='./data/lr_monthly_max.nc',
        checkpoint_path='checkpoints/simple_vit/best_model.pth',
        output_path='./data/lr_corrected_monthly_max.nc',
        model_class=SimpleViT,
        model_config=model_config,
        n_quantiles=500,
        device='cuda:1',
        batch_size=32,
        use_gpu_quantiles=True,        # â­ GPU ì‚¬ìš©
        quantile_batch_size=2000,      # â­ ë°°ì¹˜ í¬ê¸°
        restore_extremes=False
    )